from bs4 import BeautifulSoup as bs
import time
import json
import selenium.webdriver
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException
from random import choice
from multiprocessing import Pool, cpu_count
import pandas as pd
import numpy as np
from copy import copy
import argparse
from urllib.request import urlopen
import regex as re
from pathlib import Path
from typing import Union
from db_control import DBControl


class ClassAttributeError(Exception):
    def __init__(self, wrong_input, class_name: str, attribute_name: str, additional: str = None):
        """
        custom error message that will be raised in cases the user does not use a suitable values for class attributes
        :param wrong_input: the input the user choose
        :param class_name: the class the user tried to set value to
        :param attribute_name: the name of the attribute/variable the user tried to set value to
        :param additional: additional message to the one that will be generated by this class
        """
        self._message = '{} is not a valid input as {} for class {}'.format(wrong_input, attribute_name, class_name)
        if additional:
            self._message += '\n' + additional
        super().__init__(self._message)


class Driver(object):
    HEADLESS_MODE = '--headless'  # command for running in headless mode
    DEFAULT_IMPLICIT_WAIT = 50
    DRIVER_KEY, OPTIONS_KEY = 'DRIVER', 'OPTIONS'
    WEBDRIVER_BROWSERS = {'CHROME': {DRIVER_KEY: selenium.webdriver.Chrome,
                                     OPTIONS_KEY: selenium.webdriver.chrome.options.Options},
                          'FIREFOX': {DRIVER_KEY: selenium.webdriver.Firefox,
                                      OPTIONS_KEY: selenium.webdriver.FirefoxOptions}}

    def __init__(self, browser: str, implicit_wait: int = DEFAULT_IMPLICIT_WAIT,
                 executable: Union[str, Path, None] = None, *options):
        """
        Driver is an object for generating and setting selenium webdriver object with more friendly API and some
        limited options that are suitable for the task of url scrapping from instagram hashtag web pages
        :param browser: str object that represents the type of browser to use
        :param implicit_wait: an int object that represents an implicit wait time (in seconds) for the driver to
        load elements (DOM) in the required web page
        :param executable: a str, Path or None that represents a path to the driver executable file. If None It will
        be assumed that a driver was added as an OS environment variable
        :param options: represents a variable number of java script optional arguments that will be injected to the
        browser argument with selenium webdriver API
        """
        self._browser = browser
        self._driver, self._options = self._browser_dict()  # selecting suitable driver and option objects
        self._set_options(list(*options))  # setting option object
        self._executable = executable
        self._implicit_wait = implicit_wait
        self._validate_implicit_wait()
        self._set_driver()
        self._driver.implicitly_wait(self._implicit_wait)  # setting browser

    def _browser_dict(self) -> tuple:
        """
        class property that returns selenium driver and option objects matching to the browser that the
        user have selected
        :return: tuple
        """
        if self._browser not in self.WEBDRIVER_BROWSERS:
            raise ClassAttributeError(self._browser, Driver.__class__.__name__, 'browser')
        browser_dict = self.WEBDRIVER_BROWSERS[self._browser]
        return browser_dict[Driver.DRIVER_KEY], browser_dict[Driver.OPTIONS_KEY]

    def _validate_implicit_wait(self) -> None:
        """
        class function that validates that the implicit wait attribute in a natural number
        :return: None
        """
        try:
            self._implicit_wait = int(self._implicit_wait)
        except (ValueError, TypeError):
            raise ClassAttributeError(self._implicit_wait, self.__name__, 'implicit_wait',
                                      'Implicit wait must be an integer')
        if np.isnan(np.sqrt(self._implicit_wait)):
            raise ClassAttributeError(self._implicit_wait, self.__name__, 'implicit_wait',
                                      'Implicit can not hold negative values')

    def _set_options(self, options: list) -> None:
        """
        a class method for setting web driver option object
        :param options: list object that represent user requirements to be used as driver options
        """
        self._options = self._options()
        options.append(self.HEADLESS_MODE)  # adding headless mode as default
        for option in set(options):
            # using "set" in case the same option was added more than once
            try:
                self._options.add_argument(option)
            except ValueError:
                print('note that you have chosen None as an option for your browser.\nthis request will be ignored')

    def _set_driver(self) -> None:
        """
        a class method for setting web driving including all user selected options
        :return: None
        """
        if self._executable is None:  # in cases where none is given it is assumed that the driver path was
            # already added to environment path of the OS
            self._driver = self._driver(options=self._options)
            return
        try:
            self._driver = self._driver(executable_path=str(Path(self._executable).resolve()), options=self._options)
        except (WebDriverException, NotADirectoryError):
            raise ClassAttributeError(self._executable, self.__class__.__name__, 'executable')

    @property
    def driver(self) -> selenium.webdriver:
        """
        property method to get driver
        :return:  selenium webdriver object
        """
        return self._driver


class HashTagPage(object):
    SCROLL_2_BOTTOM = 'window.scrollTo(0, document.body.scrollHeight);'
    # java scrip command for scrolling to page bottom
    SCROLL_HEIGHT = 'return document.body.scrollHeight'  # java scrip command for getting scroll height
    HASHTAG_URL_TEMPLATE = 'https://www.instagram.com/explore/tags/{}/?h__a=1'
    STEP_SIZE = 0.1
    DEFAULT_MAX_WAIT_AFTER_SCROLL = 3
    DEFAULT_MIN_WAIT_AFTER_SCROLL = 1
    DEFAULT_LIMIT = np.inf
    DEFAULT_FROM_CODE = None
    DEFAULT_STOP_CODE = None

    def __init__(self, hashtag: str, driver: Driver, max_scroll_wait: int = DEFAULT_MAX_WAIT_AFTER_SCROLL,
                 min_scroll_wait: int = DEFAULT_MIN_WAIT_AFTER_SCROLL, from_code: str = DEFAULT_FROM_CODE,
                 stop_code: str = DEFAULT_STOP_CODE, limit=DEFAULT_LIMIT):
        """
        HashTagPage is an object that represents a dynamic instagram hashtag page with infinite scrolls
        :param hashtag: str that represents the hashtag page to open
        :param driver: Driver object to use in order to get hashtag page
        :param max_scroll_wait: int or None that represents maximum wait time after each scroll in hashtag page
        :param min_scroll_wait: int or None that represents minimum wait time after each scroll in hashtag page
        :param from_code: str or None that represents instagram shortcode of posts to start scraping from
        :param stop_code: str or None that represents instagram shortcode of posts to stop scraping once reached
        :param limit: int or np.inf that represents the maximum number of shortcodes to scrape
        """
        super().__init__()
        self._hashtag = hashtag
        self._driver_obj = driver
        self._scroll_pause_time_range = self._set_scroll_pause_range(min_scroll_wait, max_scroll_wait)
        self._limit = limit
        self._validate_limit()
        self._from_code, self._stop_code = from_code, stop_code
        self._shortcode_batch, self._scraped_shortcodes = [], 0
        self._previous_height = None
        self._stop_scrapping = False  # a flag that indicates if scrolling reached bottom of the web page
        self._break = False  # a flag that indicates stop automated scrolling and start collecting
        # (only relevant if from_code was provided)
        self._dbc = DBControl()

    def _set_scroll_pause_range(self, minimum: int, maximum: int) -> np.ndarray:
        """
        a class method for setting a range for selecting random wait seconds after each scroll
        :param minimum: int object that represents the minimum number of seconds to wait
        :param maximum: int object that represents the maximum number of seconds to wait
        :return: np.ndarray that represents a range of seconds
        """
        if any([not isinstance(minimum, int), minimum < 0]):
            raise ClassAttributeError(minimum, self.__class__.__name__, 'min_scroll_wait', 'min_scroll_wait should be '
                                                                                           'a positive integer')
        if any([not isinstance(minimum, int), maximum <= minimum]):
            raise ClassAttributeError(minimum, self.__class__.__name__,
                                      'max_scroll_wait', 'max_scroll_wait should be a positive integer that is larger '
                                                         'than min_scroll_wait')
        return np.arange(minimum, maximum, self.STEP_SIZE)

    def _validate_limit(self) -> None:
        """
        class method that validates that limit attribute is either positive int of np.inf
        :return:
        """
        if not any([np.isinf(self._limit), isinstance(self._limit, int)]):
            raise ClassAttributeError(self._limit, self.__class__.__name__,
                                      'limit', 'limit should be either an integer or np.inf')
        if self._limit < 0:
            raise ClassAttributeError(self._limit, self.__class__.__name__, 'limit', 'limit can not be negative')

    def _scroll(self) -> None:
        """
        a method for scrolling down a web page using selenium webdriver after.
        After each scroll page height will be recalculated and compared to that of the previous page
        In case the page height remains the it would be assumed page bottom is reached and stop scrapping flag will be
        set to True. After each scroll the system will undergo a random wait period in a given range of seconds that
        will allow the driver to load new information and avoid detection (hopefully).
        :return: None
        """
        self._driver_obj.driver.execute_script(HashTagPage.SCROLL_2_BOTTOM)  # Scroll to the bottom of the page
        time.sleep(choice(self._scroll_pause_time_range))  # random wait between each scroll
        if self.page_height == self._previous_height:  # if previous height is equal to new height
            self._stop_scrapping = True
        else:
            self._previous_height = copy(self.page_height)  # change back to deepcopy if there is an issue

    def shortcode_batch_generator(self) -> list:
        """
        generator method is used for scraping data from pages with infinite scrolling. this method works with
        the following steps:
                                0) zero stage - will keep scrolling until reaching from_code (if from code was provided)
                                1) checks if stop scraping flag was raised - if yes returns.
                                2) collects all shortcodes from current page scroll state using the
                                 _shortcode_page_scraper method
                                3) yield a batch (list) of all shortcodes collected from the current scroll state
                                4) reset/empty shortcode batch container to prepare for another round
                                5) perform another scroll
        :return: list object in every round
        """
        end_of_scrape_msg = 'scraping is done - either you scraped everything, ' \
                            'reached your limit or something went wrong.\n(lets hope it\'s the first one)'
        self.open()
        if self._from_code:
            while True:
                if self._stop_scrapping:
                    print(end_of_scrape_msg)
                    self.close()
                    return
                elif self._break:
                    break
                self._keep_scrolling()
        while True:
            if self._stop_scrapping:
                print(end_of_scrape_msg)
                self.close()
                return
            self._shortcode_page_scraper()
            self._dbc.insert_shortcodes(self._shortcode_batch)
            self._shortcode_batch = []
            self._scroll()

    def _get_shortcodes(self) -> filter:
        """
        a method for scraping for unique post shortcodes from the current scroll state of an instagram hashtag
        :return: filter
        """
        all_links = [element.get_attribute('href') for element in
                     self._driver_obj.driver.find_elements_by_tag_name('a')]
        return set(Path(url).name for url in filter(lambda link: '/p/' in link, all_links))

    def _shortcode_page_scraper(self) -> None:
        """
        a method for loading a batch of shortcodes collected with '_get_shortcodes' method and checking if
        reached scraping limit of encountered a stop code
        :return: None
        """
        for shortcode in self._get_shortcodes():
            if any([self._stop_code == shortcode, self.scraped_shortcodes >= self._limit]):
                # todo checks if reaching either stop code or limit
                self._stop_scrapping = True
                return
            self._shortcode_batch.append(shortcode)
            self._scraped_shortcodes += 1

    def _keep_scrolling(self) -> None:
        """
        a method for checking if urls contains from_code by iteration. if from code was found all urls following this
        urls in the current scroll page are loaded onto url_batch and breaks non-collecting url cycle
        :return:
        """
        links = list(self._get_shortcodes())
        for link_ind, link in enumerate(links):
            if str(self._from_code) in link:
                left_overs = links[link_ind + 1:]
                self._shortcode_batch.extend(left_overs)
                self._scraped_shortcodes += len(left_overs)
                self._break = True
                return
        self._scroll()

    @property
    def scraped_shortcodes(self) -> int:
        """
        a property method that returns the number of scrapped urls from an hashtag page
        :return: int object with the number of scraped urls
        """
        return self._scraped_shortcodes

    @property
    def page_height(self):
        """
        property method that returns the current webpage height
        :return: int object that represents the page current height of the webpage
        """
        return self._driver_obj.driver.execute_script(HashTagPage.SCROLL_HEIGHT)

    def open(self) -> None:
        """
        a method for opening an hashtag webpage
        :return: None
        """
        self._driver_obj.driver.get(HashTagPage.HASHTAG_URL_TEMPLATE.format(str(self._hashtag)))

    def close(self):
        """
        method for closing a driver at the end of the scrapping session
        :return:
        """
        self._driver_obj.driver.close()


class MultiScraper(object):
    pass

# constants for post scraping function:
POST_KEY_WORD = 'window._sharedData = '


## todo built a class for submitting: step 1 collecting short codes, step 2, checking with DB for duplicates,
## todo step3 getting updated list of shorts

def post_scraping(url: str):
    try:
        source = urlopen(url)
        body = bs(source, 'html.parser').find('body')
        script = body.find('script', text=lambda t: t.startswith(POST_KEY_WORD))
        page_json, = filter(None, script.string.rstrip(';').split(POST_KEY_WORD, 1))
        posts, = json.loads(page_json)['entry_data']['PostPage']
        return posts['graphql']
    except Exception as e:
        print(e)
        return json.loads('{}')


def normalize(json_record: json):
    return pd.json_normalize(json_record)


def multi_scraper(hashtag_page: HashTagPage, available_cpus: int):
    json_records = []
    try:
        for url_batch in hashtag_page.shortcode_batch_generator():
            with Pool(processes=available_cpus) as p:
                json_records.extend(p.map(post_scraping, url_batch))
                print('done scrapping a total of {} posts. so far...'.format(hashtag_page.scraped_shortcodes))
    except Exception as general_error:    # several web related exception
        print('an unexpected error has occurred\n{}'.format(general_error))
    finally:
        with Pool(processes=available_cpus) as normalization:
            pandas_records = normalization.map(normalize, json_records)
        return pd.concat(pandas_records).astype(str).drop_duplicates().reset_index(drop=True)


def get_hashtags(text):
    p = re.compile(r'#(\w*)')
    return p.findall(text)


# constants for argparse

#DEFAULT_FIELDS = ['id', 'shortcode', 'timestamp', 'photo_url', 'post_text', 'preview_comment', 'ai_comment',
#                  'like_count', 'location_name', 'owner_profile_pic_url', 'owner_username', 'owner_full_name',
#                  'owner_edge_followed_by_count', 'is_ad']

COL_NAME_DICT = {'shortcode_media.__typename': 'type',
                 'shortcode_media.id': 'id',
                 'shortcode_media.shortcode': 'shortcode',
                 'shortcode_media.dimensions.height': 'dim_height',
                 'shortcode_media.dimensions.width': 'dim_width',
                 'shortcode_media.display_url': 'photo_url',
                 'shortcode_media.accessibility_caption': 'ai_comment',
                 'shortcode_media.is_video': 'is_video',
                 'shortcode_media.edge_media_to_tagged_user.edges': 'user_details',
                 'shortcode_media.edge_media_to_caption.edges': 'post_text',
                 'shortcode_media.edge_media_to_parent_comment.count': 'comment_count',
                 'shortcode_media.edge_media_to_parent_comment.edges': 'comments',
                 'shortcode_media.edge_media_preview_comment.count': 'preview_comment_count',
                 'shortcode_media.edge_media_preview_comment.edges': 'preview_comment',
                 'shortcode_media.comments_disabled': 'comments_disabled',
                 'shortcode_media.taken_at_timestamp': 'timestamp',
                 'shortcode_media.edge_media_preview_like.count': 'like_count',
                 'shortcode_media.location.id': 'location_id',
                 'shortcode_media.location.has_public_page': 'location_has_public_page',
                 'shortcode_media.location.name': 'location_name',
                 'shortcode_media.location.slug': 'location_slug',
                 'shortcode_media.location.address_json': 'location_json',
                 'shortcode_media.owner.id': 'owner_id',
                 'shortcode_media.owner.is_verified': 'owner_is_verified',
                 'shortcode_media.owner.profile_pic_url': 'owner_profile_pic_url',
                 'shortcode_media.owner.username': 'owner_username',
                 'shortcode_media.owner.full_name': 'owner_full_name',
                 'shortcode_media.owner.is_private': 'owner_is_private',
                 'shortcode_media.owner.is_unpublished': 'owner_is_unpublished',
                 'shortcode_media.owner.pass_tiering_recommendation': 'tiering_recommendation',
                 'shortcode_media.owner.edge_owner_to_timeline_media.count': 'owner_media_count',
                 'shortcode_media.owner.edge_followed_by.count': 'owner_edge_followed_by_count',
                 'shortcode_media.is_ad': 'is_ad',
                 'shortcode_media.edge_sidecar_to_children.edges': 'multiple_photos',
                 'shortcode_media.video_duration': 'video_duration',
                 'shortcode_media.product_type': 'product_type'}


DEFAULT_FIELDS = ['shortcode', 'timestamp', 'ai_comment', 'like_count', 'location_name', 'owner_username',
                  'owner_edge_followed_by_count', 'owner_media_count', 'comment_count']


def arg_parser():
    parser = argparse.ArgumentParser(prog='coronagram.py', description=f'#### Instagram Scrapping ####\n',
                                     epilog=f'List of possible fields to choose:\n'
                                            f'{" ".join(list(COL_NAME_DICT.values()))}',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('tag', type=str, help='Choose a #hashtag')
    parser.add_argument('-l', '--limit', type=int, default=np.inf, help='number of posts to scrape')
    parser.add_argument('-f', '--fields', nargs='*', type=str, help='Choose posts fields to keep. '
                                                                    'If no -f, default fields will be printed. '
                                                                    'If -f only without fields, all json fields will be'
                                                                    f' printed. Defaults fields are:'
                                                                    f' {" ".join(DEFAULT_FIELDS)}')
    parser.add_argument('-b', '--browser', type=str, default='CHROME', help='browser choice to be used by selenium. '
                                                                            'supported browsers:\t{}'
                        .format('|'.join(Driver.WEBDRIVER_BROWSERS.keys())))
    parser.add_argument('-e', '--executable', type=str, default=None, help='a path to the driver executable file. '
                                                                           'If none is given it will be assumed that '
                                                                           'the driver was added and available as an '
                                                                           'OS environment variable')
    parser.add_argument('-c', '--cpu', type=int, default=cpu_count() - 1,
                        help='number of cpu available for multiprocessing')
    parser.add_argument('-fc', '--from_code', type=str, help='url shortcode to start scraping from')
    parser.add_argument('-sc', '--stop_code', type=str, help='url shortcode that when reach will stop scrapping')
    parser.add_argument('-i', '--implicit_wait', type=int, default=50, help='implicit wait time for '
                                                                                       'webdriver')
    # test that validate that this value is a non negative int
    parser.add_argument('-do', '--driver_options', type=str, default=[], help='ava script optional arguments that will '
                                                                              'be injected to the browser argument '
                                                                              'with selenium webdriver API',
                        action='append')
    parser.add_argument('-mn', '--min_scroll_wait', type=int, default=3,
                        help='minimum number of seconds to wait after each scroll')
    parser.add_argument('-mx', '--max_scroll_wait', type=int, default=5,
                        help='maximum number of seconds to wait after each scroll')

    args = parser.parse_args()


    json_fields = []
    if args.fields is None:
        fields = DEFAULT_FIELDS
    else:
        fields = args.fields

    for field in fields:
        json_fields.append(field)

    if args.cpu <= 0:
        args.cpu = 1

    return args.tag, args.limit, json_fields, args.browser, args.executable, args.cpu, args.from_code, args.stop_code, \
           args.implicit_wait, args.driver_options, args.min_scroll_wait, args.max_scroll_wait


def main():
    tag, limit, fields, browser, executable, cpu, from_code, stop_code, implicit_wait, driver_options, \
    min_scroll_wait, max_scroll_wait = arg_parser()
    driver = Driver(browser, implicit_wait, executable, driver_options)
    hashtag_page = HashTagPage(tag, driver, max_scroll_wait, min_scroll_wait, from_code, stop_code, limit)
    records = multi_scraper(hashtag_page, cpu)

    records = records.rename(columns=COL_NAME_DICT)
    records['hashtag'] = records['post_text'].apply(get_hashtags)  # adding hashtag tags
    records = records.loc[:, fields]
    print(records)


if __name__ == '__main__':
    main()
